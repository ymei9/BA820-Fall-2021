{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP0WB9Lk90ls"
      },
      "outputs": [],
      "source": [
        "# LEARNING GOALS\n",
        "#\n",
        "#                 - text as a datasource\n",
        "#                 - cleaning text\n",
        "#                 - basic eda\n",
        "#                 - Doc Term Matrix representation by hand\n",
        "#                 - The intuition behind working with text before jumping into tools that abstract this away\n",
        "#                 - how text can be used in ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJJaigOy9ylQ"
      },
      "outputs": [],
      "source": [
        "######################################## some helpful resources:\n",
        "# https://www.w3schools.com/python/python_regex.asp\n",
        "# https://docs.python.org/3/library/re.html\n",
        "# https://www.debuggex.com/cheatsheet/regex/python\n",
        "# https://www.shortcutfoo.com/app/dojos/python-regex/cheatsheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAuyoLsOLJkF"
      },
      "outputs": [],
      "source": [
        "# installs\n",
        "# ! pip install newspaper3k\n",
        "# ! pip install spacy\n",
        "# ! pip install wordcloud\n",
        "# ! pip install emoji\n",
        "# ! pip install nltk\n",
        "# ! pip install scikit-plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwjWwq_SLRq7"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplot\n",
        "from newspaper import Article\n",
        "\n",
        "# some \"fun\" packages\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "\n",
        "import re\n",
        "\n",
        "# new imports\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  \n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hhnSSlrLY6G"
      },
      "outputs": [],
      "source": [
        "# a simple corpus\n",
        "a = ['I like turtles!',\n",
        "     'You like hockey and golf ',\n",
        "     'Turtles and hockey ftw',\n",
        "     'Python is very easy to learn. üêç',\n",
        "     'A great resource is www.spacy.io',\n",
        "     ' Today is the Feb 22, 2021 !           ',\n",
        "     '@username #hashtag https://www.text.com',\n",
        "     'BA820 ']\n",
        "\n",
        "df = pd.DataFrame({'text':a})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBpxLkHfMqiv"
      },
      "outputs": [],
      "source": [
        "## QUICK QUESTION\n",
        "##        What do you see about the data being brought in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWoJILfeNMvG"
      },
      "outputs": [],
      "source": [
        "## we can always get the values back\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv7wRHAFMFqe"
      },
      "outputs": [],
      "source": [
        "# quick review of some of the string funcationality\n",
        "\n",
        "# capitalize or change case\n",
        "# upper, lower, strip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM6ebG30MFkj"
      },
      "outputs": [],
      "source": [
        "# we can detect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRTvt3OtMFdx"
      },
      "outputs": [],
      "source": [
        "# remember python is case sensitive!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSRq0Kz3Pqxd"
      },
      "outputs": [],
      "source": [
        "# we can replace anything that matches a pattern\n",
        "# but we will come back to patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xlAnbp9EajW"
      },
      "outputs": [],
      "source": [
        "# we can look at the length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQQE-097RVu5"
      },
      "outputs": [],
      "source": [
        "#### NOTE:\n",
        "##      but look at above, what do you notice about the lengths calculated?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RroUtCMhSd3b"
      },
      "outputs": [],
      "source": [
        "# lets look at the values directly again for the last entry\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlNyr7fREdDd"
      },
      "outputs": [],
      "source": [
        "# lets count characters and numbers\n",
        "\n",
        "# df.text.str.count(\"[a-zA-Z0-9]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-UfYjJJLRxS"
      },
      "outputs": [],
      "source": [
        "## regex\n",
        "## https://www.regular-expressions.info/quickstart.html\n",
        "##\n",
        "## https://regex101.com/\n",
        "##\n",
        "## [a-z] will match a single letter lowercase a to z\n",
        "## [A-Z] will match a single letter uppercase A to Z\n",
        "## [a-zA-Z0-9] will match a single character that is alphanumeric\n",
        "## ^ matches a pattern at the start\n",
        "## $ matches a pattern at the end\n",
        "## + will match a pattern one or more times\n",
        "## * will match 0 or more\n",
        "## .* will match everything (dot is any character)\n",
        "## {3} match pattern exactly 3 times\n",
        "## {2,4} match a pattern 2 to 4 times\n",
        "## {3, } match a pattern 3 or more times\n",
        "## | allows us to specify \"or\"\n",
        "## so much more including special patterns and shortcuts\n",
        "## \\d for a digit\n",
        "## \\w for word characters\n",
        "## [:punct:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCqqYOMjVWOd"
      },
      "outputs": [],
      "source": [
        "# only print out entries if the pattern matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dBwYTI1VWHX"
      },
      "outputs": [],
      "source": [
        "# again, case sensitive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCazcb-0VWCJ"
      },
      "outputs": [],
      "source": [
        "# we can use \"OR\" logic\n",
        "\n",
        "# FIND = df.text.str.contains(\"tu+|BA\")\n",
        "# df.text[FIND]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HS_lr7sVT5W"
      },
      "outputs": [],
      "source": [
        "# matches\n",
        "\n",
        "# FIND = df.text.str.contains(\"hock{1}\")\n",
        "# df.text[FIND]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKM7jqMNXwph"
      },
      "outputs": [],
      "source": [
        "# matches\n",
        "\n",
        "# FIND = df.text.str.contains(\"\\d\")\n",
        "# df.text[FIND]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtB6Rz4ZX6OZ"
      },
      "outputs": [],
      "source": [
        "# special characters anywhere\n",
        "\n",
        "# FIND = df.text.str.contains(\"\\d\")\n",
        "# df.text[FIND]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wOgQFCvsEH0"
      },
      "outputs": [],
      "source": [
        "# extract username or hashtag\n",
        "# uses not whitespace character, repeating 1+\n",
        "# df.text.str.findall('@\\S+|#\\S+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRGdAfkXsNAZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTmquoontJB1"
      },
      "outputs": [],
      "source": [
        "# you may get an error around capture groups\n",
        "# a group is in parentheses\n",
        "\n",
        "# df.text.str.extract('@\\S+')    #<----- error\n",
        "# df.text.str.extract('(@\\S+)')   # <- works as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI68pxAnyGxS"
      },
      "outputs": [],
      "source": [
        "########################################### Regex Exercise\n",
        "##\n",
        "##  ~ 15 minutes to get back into the flow of text and the skills we saw earlier\n",
        "##\n",
        "##  There is a story (article) at the URL below\n",
        "##  Let's go back to some of the material that we covered in BA765\n",
        "##  Pull in the article into a format that you can analyze the full story\n",
        "##  Below are various questions for you to consider as your parse the information\n",
        "##  This helps us think about the elements that exist in text, and how \n",
        "##  we can consider extracting features from text\n",
        "\n",
        "## HINT/TIP!  Newspaper is great for text parsing of news sites and such\n",
        "\n",
        "URL = \"https://news.yahoo.com/world-first-airport-therapy-pig-131238990.html\"\n",
        "\n",
        "\n",
        "# get the article and parse\n",
        "# article = Article(URL)\n",
        "# article.download()\n",
        "# article.parse()\n",
        "\n",
        "# the text into a pandas dataframe\n",
        "# txt = [article.text]\n",
        "# txt = pd.DataFrame({\"txt\":txt})\n",
        "\n",
        "# what do we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJg9Pu2T-Oan"
      },
      "outputs": [],
      "source": [
        "################################################\n",
        "## ok, lets start to think of text as a dataset\n",
        "## we will smart small, and then start to increment\n",
        "##\n",
        "\n",
        "docs = ['I like golf', \n",
        "        'i like hockey',\n",
        "        'Data science is a super skill!']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNyrcUJAkzH5"
      },
      "outputs": [],
      "source": [
        "## Thought Exercise:\n",
        "##    Our datasets that we typically see take the shape of:\n",
        "##    Rows =    Observations\n",
        "##    Columns = Attributes about those Observations\n",
        "## \n",
        "##    How can we map this to text?\n",
        "##\n",
        "##    Rows =    A document (the source, we will talk about this)\n",
        "##    Columns = The words in the document\n",
        "##   \n",
        "##    Above can be referred to as a Document Term Matrix, or Document Feature Matrix\n",
        "##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHn2tfeQTGH1"
      },
      "outputs": [],
      "source": [
        "# remember split?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHOlKQAxTGTB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkz56zw1TGYE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpgwsaQsTGb9"
      },
      "outputs": [],
      "source": [
        "# lets do this in a dataframe\n",
        "\n",
        "# df = pd.DataFrame({'doc':docs})\n",
        "# df\n",
        "# df['tokens'] = df.doc.str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWB8hamtUIpu"
      },
      "outputs": [],
      "source": [
        "# if we really wanted to (or had to), we \n",
        "# have the python chops to make this a doc/term matrix\n",
        "\n",
        "# step 0, just the tokens but keep as a dataframe\n",
        "# tdf = df[['tokens']]\n",
        "\n",
        "# step 1: melt it via explode\n",
        "# tdf_long = tdf.explode(\"tokens\")\n",
        "# tdf_long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WL7E1O5UZz1"
      },
      "outputs": [],
      "source": [
        "# step 3: back to wide for a dtm\n",
        "\n",
        "# tdf_long['value'] = 1\n",
        "# dtm = tdf_long.pivot_table(columns=\"tokens\", \n",
        "#                            values=\"value\", \n",
        "#                            index=tdf_long.index,\n",
        "#                            aggfunc=np.count_nonzero)\n",
        "\n",
        "\n",
        "\n",
        "# dtm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqmgKNUKUag1"
      },
      "outputs": [],
      "source": [
        "# lets review what we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wzl5xygVcGe"
      },
      "outputs": [],
      "source": [
        "## Quick thought exercise:\n",
        "##      What do you notice about our tokenized dataset\n",
        "##      What about the values?  What would you change?\n",
        "##   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbtVFg-GbPB_"
      },
      "outputs": [],
      "source": [
        "################ YOUR TURN\n",
        "##  from the topics table on big query (datasets.topics), bring in just the text column\n",
        "##  Make the text lowercase\n",
        "##  Tricky!! remove punctuation if you can (keep just letters and numbers)\n",
        "##  get the text into a long form where each token is a row in the dataframe\n",
        "##  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYN7nRehodBR"
      },
      "outputs": [],
      "source": [
        "################################### Text EDA\n",
        "## lets get a cleaned dataset in long format\n",
        "\n",
        "# get the data from topics_long\n",
        "\n",
        "# remember!  use your billing project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNHwrZjdpRL6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# what do we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z89aM6aSpq7z"
      },
      "outputs": [],
      "source": [
        "### ## in case, this is what some in the R space might call a tidy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu_CLhyvpyNT"
      },
      "outputs": [],
      "source": [
        "# lets just recenter what we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykPFhGOEp7Rp"
      },
      "outputs": [],
      "source": [
        "# start simple, how many unique tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwMoA-gyqUT5"
      },
      "outputs": [],
      "source": [
        "# what are the top 15 words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG3mGdYLqZN8"
      },
      "outputs": [],
      "source": [
        "# we could even plot this if we had to\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb5V-GOvqn-f"
      },
      "outputs": [],
      "source": [
        "# lets plot all word frequencies\n",
        "\n",
        "# (tl\n",
        "#  .token\n",
        "#  .value_counts()\n",
        "#  .plot(kind=\"line\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3xIL8gDq-ye"
      },
      "outputs": [],
      "source": [
        "## Plot is a little misleading, but the idea is the most\n",
        "## common words occur at rates much larger than other words\n",
        "##\n",
        "## This is where the typical discussion of Zipfs Law is introduced\n",
        "##  Frequency of a word is inversely proportional to its rank\n",
        "## Most common word appears approx. twice as often as the 2nd most common word, etc.\n",
        "## Most texts stop at this point, but I linked to a good article on Medium on the \"why\"\n",
        "## TLDR: - we can make inferences about the probability of seeing a given word with a corpus of N size of words\n",
        "##       - it matters because when we define our vocab size, this can help us think about sample sizes for our model\n",
        "##       - this is where big data/deep learning comes into play, but we can go A LONG way prior to getting there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYnbvzRbtwup"
      },
      "outputs": [],
      "source": [
        "# quick review: what does our data look like again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feXlvFGMu8sJ"
      },
      "outputs": [],
      "source": [
        "# summary stats\n",
        "\n",
        "# N_TOPICS = tl.topic.nunique()\n",
        "# N_DOCS = tl.id.nunique()\n",
        "# print(f\"number of unique topics: {N_TOPICS}\")\n",
        "# print(f\"number of documents in the corpora: {N_DOCS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7BQOmohtDVa"
      },
      "outputs": [],
      "source": [
        "# we could even summarize stats about each document\n",
        "# token_stats = tl.groupby(\"token\", as_index=False)\n",
        "# token_stats = token_stats.agg({'id':['size', 'nunique'],'topic':'nunique'})\n",
        "\n",
        "# clean cols to start\n",
        "# new_cols = ['_'.join(t).rstrip(\"_\") for t in token_stats.columns]\n",
        "# token_stats.columns = new_cols\n",
        "\n",
        "# new summary cols\n",
        "# token_stats['pct_docs'] = token_stats.id_nunique / N_DOCS  ## token occurrence % all docs\n",
        "# token_stats['pct_topics'] = token_stats.topic_nunique / N_TOPICS  ## token coverage across all topics\n",
        "# token_stats['n_per_doc'] = token_stats.id_size / token_stats.id_nunique ## token frequency / doc\n",
        "# token_stats['pct_tokens'] = token_stats.id_size / token_stats.id_size.sum() ## % of all token occurrences\n",
        "# token_stats['rank'] = token_stats.id_size.rank(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlqJoo6sScgX"
      },
      "outputs": [],
      "source": [
        "# what do we have now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3SjyNM_uXEY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-dtVgJYadsL"
      },
      "outputs": [],
      "source": [
        "######################## DTM\n",
        "##\n",
        "## Now lets take the data and put it back into a DTM \n",
        "## Keep only the top 500 tokens\n",
        "##\n",
        "## we will use the long dataformat, not the summary frame above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChujAeEdfUdg"
      },
      "outputs": [],
      "source": [
        "# the tokens in our summary frame\n",
        "\n",
        "# TOP_N = 1000\n",
        "# top_toks = (token_stats\n",
        "#             .sort_values(\"id_size\", ascending=False)\n",
        "#             .iloc[:TOP_N, :][['token', \"id_size\", \"pct_tokens\"]])\n",
        "# top_toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVM88pIzgoqx"
      },
      "outputs": [],
      "source": [
        "# how many tokens and % of all words total\n",
        "# PCT_TOTAL = top_toks.pct_tokens.sum()\n",
        "\n",
        "# print(f\"The top {TOP_N} tokens account for {PCT_TOTAL} all tokens logged.  \\nIf 10 docs have 10 words, this is freq(token)/10*10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCtAEFbRdVPv"
      },
      "outputs": [],
      "source": [
        "# back to a dtm\n",
        "\n",
        "# limit to the top N from above\n",
        "# TOP_N_TOKENS = list(top_toks.token.values)\n",
        "\n",
        "# # keep just the data that we need, in steps\n",
        "# tl2 = tl.loc[tl.token.isin(TOP_N_TOKENS), :]\n",
        "# tl3 = tl2.loc[:, [\"pos\", \"token\", \"id\"]]\n",
        "\n",
        "# # the construction\n",
        "# dtm = tl3.pivot_table(columns=\"token\", \n",
        "#                       values=\"pos\",  # just to summarise\n",
        "#                       index=tl2.id,  # the document id from the original dataset\n",
        "#                       aggfunc=np.count_nonzero)  # how many times did it appear per doc id\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFqVCqJleTAo"
      },
      "outputs": [],
      "source": [
        "# what do we have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfdlCCx0gePY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovxnjs68lRFP"
      },
      "outputs": [],
      "source": [
        "######################################  Quick question for you\n",
        "## what are your observations about above?\n",
        "##\n",
        "## \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHnxFYPull9s"
      },
      "outputs": [],
      "source": [
        "## replace missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leG6FSbnmZUW"
      },
      "outputs": [],
      "source": [
        "############ NOTE:\n",
        "## \n",
        "## Truthfully, this a sparse represenation and one where we are adding space just til have a full matrix.  \n",
        "## But lets just start here, because this format allows us to apply ML tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQBhFljHmq5A"
      },
      "outputs": [],
      "source": [
        "#################################### Lets predict the category!\n",
        "##\n",
        "## we now have a dataset that can be used to fit a ML model.  \n",
        "## the quality of the models and how we think about ML tasks is all about the data\n",
        "## let's start with this framing for intuition\n",
        "##\n",
        "##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udxGSGrAnKEA"
      },
      "outputs": [],
      "source": [
        "# imports - violating my rule of the thumb\n",
        "\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpQPpKxFuvWg"
      },
      "outputs": [],
      "source": [
        "# we need to get the label per doc\n",
        "# we were given a long dataset, so lets not assume\n",
        "# just groupby and get the data\n",
        "\n",
        "# assumption that id and topic always exist, so just need to isolate the values\n",
        "\n",
        "# labels = (tl\n",
        "#           .loc[:, [\"id\", \"topic\"]]\n",
        "#           .drop_duplicates()\n",
        "#           .topic\n",
        "#           .to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZTkhThano-q"
      },
      "outputs": [],
      "source": [
        "# lets build the datasets for the model\n",
        "# X = dtm.copy()\n",
        "# y = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ_uhzCgyDtj"
      },
      "outputs": [],
      "source": [
        "# confirm we have the same thing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebd6xhbPBozJ"
      },
      "outputs": [],
      "source": [
        "# split the data\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=820)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkaye_UrovCU"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "\n",
        "# tree = DecisionTreeClassifier(max_depth=5, min_samples_split=30, min_samples_leaf=15)\n",
        "# tree.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uca_rPMDyhdP"
      },
      "outputs": [],
      "source": [
        "# fit metrics on test\n",
        "\n",
        "# preds = tree.predict(X_test)\n",
        "# ctable = metrics.classification_report(y_test, preds)\n",
        "# print(ctable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6U9hpQ-yoTI"
      },
      "outputs": [],
      "source": [
        "# confusion matrix from skplot\n",
        "# can see where the model isn't sure\n",
        "\n",
        "# skplot.metrics.plot_confusion_matrix(y_test, preds, \n",
        "#                                      figsize=(7,4), \n",
        "#                                      x_tick_rotation=90 )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf56BZoIzFiF"
      },
      "outputs": [],
      "source": [
        "#################################### REVIEW\n",
        "##\n",
        "## - normal text form -> a DTM\n",
        "## - we saw that tokenizing, and the logic we apply, matters (case, punctuation)\n",
        "#     will we see even more example\n",
        "## - if we had to, we can parse text into a format for machine learning\n",
        "## - nothing stopping us from passing in a count-based dtm into a ML model!\n",
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9aezQCJzGUF"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "########################################### Team Challenge\n",
        "############################################################\n",
        "\n",
        "## review the slides at the end of this module\n",
        "## predict spam\n",
        "## objetive =  based on accuracy\n",
        "## only input is text, but you can derive features\n",
        "## limited time, but how do you maximize your time (and the model?)\n",
        "## HINTS:\n",
        "##        start small, simple models\n",
        "##        iterate and see how you do against the leaderboard\n",
        "##        code above helps you with the core mechanics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "07-Text-1",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
