# -*- coding: utf-8 -*-
"""10 - Text - Summarization and Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AI7HA5JTKkIkSRhPgsj2-f4q8bBi3Bof
"""

##############################################################################
## Fundamentals for pratical Text Analytics - classification tasks and applications
##
## Learning goals:
##                 - reinforce text as a robust dataset via language modeling and understanding
##                 - python packages for handling our corpus for these exploratory tasks
##                 - a little more spacy
##                 - text classification discussions and include it in other pipelines
##############################################################################

# installs 
! pip install newspaper3k
! pip install -U spacy
! pip install afinn
! pip install spacytextblob
! pip install pysrt
! pip install textdescriptives
! pip install tokenwiser

## you may need to restart the colab kernel

# imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# text imports
import spacy
from spacy import cli
from spacytextblob.spacytextblob import SpacyTextBlob
import textdescriptives as td
import tokenwiser

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

import gensim

from afinn import Afinn
from newspaper import Article

import warnings
warnings.filterwarnings("ignore")



##################################### WARMUP Exercise
##################################### 
## there is a file located below: 
##     https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt
## there is also some started code below
## calculate the sentiment over the course of the movie script (Shrek)
## plot the sentiment arc over the movie
##
## Some started code has been provided to you

# 0. get the file - the file in the browser will auto download
#    just make sure you have the file in your working directory

# or if on colab
# ! wget https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt

# 1. get the file and parse

# import pysrt
# subs = pysrt.open('Shrek-2001.srt', encoding='iso-8859-1')

# the first message -- each entry has a text attribute we can use to build our corpus
# print(subs[0].text)

# make a corpus (list of texts)







##################################### Quick Recap - Tokenization
##################################### 
## nltk has built in tokenizers
## lets extend what we saw in the context of a sklearn flow

# from nltk.tokenize import word_tokenize, TweetTokenizer

# a simple corpus

# corpus = ['Brock has a dog named Bodhi', 
#           '@brocktibert loves to write code in #pydata']

# word tokenizer is basic
# this is helpful as a starting point -- we will see some tools want a list of lists, with each entry considered a token

# of course, we could always go to base python

# we could use TweetTokenizer

# roll it into sklearn

# def parser(text):
#   social = TweetTokenizer()
#   return social.tokenize(text)



##################################### Your turn
###
### tokenize the shrek movie script corpus using word_tokenize from nltk
### TIPS:
###      try word_tokenize on a single piece of text
###      roll your own function to tokenize
###      use count vectorizer
###      put it back into a dataframe for review --- this isn't usually necessary, but helps with the intuition of what is happening!











### The big point here is that a number of tools may look
### for our a list of lists, including scikit learn.
### I bring this up for spacy, where the parsed doc is an object
### but if we have to, we can simply coerce to that format.



##################################### Key Words in Context - Concordance
###
### powerful tool to look at a set of text (full corpus) and look for
### words before/after
###
### helpful for eda, look for patterns to help support data annotation, etc.

# get the data -- airline tweets

# SQL = "SELECT * FROM `questrom.datasets.airlines-tweets`"



# text -- acts like a big corpus
# from nltk import Text

# get the text into the correct format

# tweet_text = tweets.text.tolist()
# print(len(intents_text))

# put into a corpus (as if it were 1 big file, ignoring that we have intents)

# corpus = " ".join(tweet_text)

# tokenize

# tokens = nltk.word_tokenize(corpus)
# len(tokens)

# put the corpus into a Text object.  
# some nice features when we consider the corpus a blob of text that lacks 
# structure like sentences, or by user, etc.  Just the text combined like a book chapter.

# text = Text(tokens)

# look for the context of words
# Key work in context, or concordance

## flight / delay / service / baggage / boston
# text.concordance("", width=80, lines=10)



##################################### Text Exploratory Descriptive Stats
##################################### https://github.com/HLasse/TextDescriptives
##
## desctriptive stats for a piece of text
## plays well with spacy and dataframes
## think of this as a way to featurize our text beyond tokenization
## depending on our tasks, this could help with downstream classification tasks

# download the small spacy language model

# model = "en_core_web_sm"
# cli.download(model)
# nlp = spacy.load(model)

"""![](https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)"""

# what do we have

# lets add additional components

# nlp.add_pipe("spacytextblob")
# nlp.add_pipe("textdescriptives")

# review the robust NLP pipeline

# lets parse our tweets dataset from earlier

# lets take a sample of 100 to help keep this "fast"
# worth noting that real datasets and balancing runtimes and performance can be at odds at times

# docs = nlp.pipe(tweets.text.sample(100, random_state=820))

# extract the summary stats from the tweet docs
# every tweet is being run through the nlp pipeline





# review the docs -- summarizes stats for tokens and sentences, but also some complexity estimates
# https://hlasse.github.io/TextDescriptives/readability.html#
# whats interesting about this is these could be features depending on your task
# news or clickbait
# valid news or social post
# news source (tabloid versus the NYT or BBC)
# I have even seen readability considered for SEO/content reviews on websites

# what is the average grade using gunning_fog

# researchers/linguists try to quantify complexity/grade level
# take two approaches, see if there is relative agreement
# variance tends to increase > 10



##################################### Your turn
##

## use article to grab entries
## source, and generate a pandas dataframe with readability comparisons 
## establish concepts as features for downstream tasks, potentially

# URL1 = "https://www.bbc.com/culture/article/20211117-why-u2s-one-is-the-ultimate-anthem"
# URL2 = "https://www.theonion.com/dollar-dangling-from-fishing-line-sure-does-look-entici-1848019332"
# URL3 = "https://www.nytimes.com/2021/11/20/world/china-congo-cobalt-explained.html"

# general flow
# article = Article()
# download and then parse methods on the article
# use the .text attribute









## FUN PRACTICE AND THOUGHT EXERCISE - SHREK MOVIE TRANSCRIPT
## You could practice and think about readbility by using the Shrek transcript
## lets think about the reading model:  https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index
## are we supplying a proper dataset if we use the Shrek movie transcript?  does it make sense directionally, use as index and not absolutes?



##################################### Intent Classification within spacy
## I mentioned everything will start to show a concept of frameowrks
##
## we already saw spacy has pipelines
## we can also extend the objects, in this case, the Doc object
## we will set an extension on a Doc that will apply the

# tokenwiser is a fantastic abstraction layer for some pre-processing tasks
# it also created the bridge for us to plug sklearn models into spacy
# as a doc extension

# from tokenwiser.extension import sklearn_method

# get the data training dataset for sms spam

# SQL = "SELECT * from `questrom.SMSspam.train`"



# lets build a pipeline from sklearn to classify the messages

# steps = [("cv", CountVectorizer()), 
#          ("clf", DecisionTreeClassifier(max_depth=7, 
#                                         min_samples_leaf=15, 
#                                         min_samples_split=30, 
#                                         random_state=820))]

# pipe = Pipeline(steps)

# fit the model - use spacy to

# play with the model

# review the classes

# lets set this as a property

# from spacy.tokens import Doc
# Doc.set_extension("spam", getter=sklearn_method(pipe))

# lets play around





# we could always roll our own without tokenwiser attribute 

# def spam_cat(doc):
#   label = pipe.predict([doc.text[0]])
#   label = str(label[0])
#   return label



##################################### Lets zoom out
##
## we are fitting a classifier with python
## we are showing that we COULD include it in a spacy pipeline.  This is helpful is spacy is at the core of a stack
## spacy is very powerful, as we will continue to use spacy for NER tasks and embeddings
## 
## Classification has a broad range of applications
## news categories for content sorting
## customer service requests
## chatbots (what is the user asking for)    <------ Rasa, in part, uses spacy to help build chatbots
## topic classification   <-------- There is topic modeling, but I have always felt like ML classifiation task are better for all sorts of problems
##   --- we will see a form of this when we jump into embedding representations of text
##
##



##################################### a domain-specific ML approach
## 
## Work in your capstone groups - you should be working in teams, not by yourself
##
## When it comes to things like sentiment, sometimes its better to annotate and build our own classifier
## 
## What does this mean?
## 1. collect a dataset
## 2. annotate the data with our own business rules
##  --------> Label studio?
## 3. we can use some of the tools above 
## ---------> generate a score, define a threshold, give labels
## ---------> fit a model on labels
## ---------> review, iterate, review, iterate
##
## Why build our own?
## 
## - out of the box generalize (thats a theme you have heard me say)
## - domain specific words may not be captured
## - also, sarcasm is hard to detect even with modifier approaches like TextBlob or Vader
##
## 
## there is an airlines tweets dataset on biq query
## bring in questrom.datasets.airlines-tweets
## just the tweet_id, airline_sentiment, airline, and text columns
##
## NOTES:  You can use any model you like
##         You DO NOT have to use spacy, just highligting that these tools can be play well together
## 
## after you make predictions, are there any differences by airline?

